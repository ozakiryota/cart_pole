{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "cartpole_qlearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOqQsZWYadvNpevhDmSgpC+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ozakiryota/cart_pole/blob/main/cartpole_qlearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzGR6DXx5hxW"
      },
      "source": [
        "# Cart-Pole\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NztiGF0Ld3ET"
      },
      "source": [
        "Install OpenAI Gym (https://gym.openai.com)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTnd9XdOQ0JA",
        "outputId": "e4dd681c-e2a1-4083-a195-865eb7d96ddb"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WatqsT0I5lxF"
      },
      "source": [
        "Install the packages for visualizing Gym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uo7Lrbp5Sqod",
        "outputId": "1bd35d7c-751c-4bc0-c1b3-ca8b94241229"
      },
      "source": [
        "!apt update\n",
        "!apt install xvfb\n",
        "!pip install pyvirtualdisplay"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Get:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Ign:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:15 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,811 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,867 kB]\n",
            "Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [928 kB]\n",
            "Fetched 5,878 kB in 3s (2,137 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "37 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 784 kB of archives.\n",
            "After this operation, 2,270 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.9 [784 kB]\n",
            "Fetched 784 kB in 1s (853 kB/s)\n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 155222 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.9_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-2.2-py3-none-any.whl (15 kB)\n",
            "Collecting EasyProcess\n",
            "  Downloading EasyProcess-0.3-py2.py3-none-any.whl (7.9 kB)\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
            "Successfully installed EasyProcess-0.3 pyvirtualdisplay-2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cF70T7rbdLXM"
      },
      "source": [
        "## Q-learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9MoZlOvenEx"
      },
      "source": [
        "Import Gym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDrypO0FZ9IG"
      },
      "source": [
        "import gym"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6r-cLs8ej61"
      },
      "source": [
        "Import the packages for visualizing Gym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8zm1ycgaE8K"
      },
      "source": [
        "import base64\n",
        "import io\n",
        "from gym.wrappers import Monitor\n",
        "from IPython import display\n",
        "from pyvirtualdisplay import Display"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgvZ1f4MccCJ"
      },
      "source": [
        "Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a3R-TE_gaS1"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Htu4VPF8cflL"
      },
      "source": [
        "Brain class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMFWoAE1RIRe"
      },
      "source": [
        "class Brain:\n",
        "\tdef __init__(self, num_states, list_state_range, list_state_reso, num_actions, gamma, r, lr):\n",
        "\t\tself.num_states = num_states\n",
        "\t\tself.list_state_range = list_state_range\n",
        "\t\tself.list_state_reso = list_state_reso\n",
        "\t\tself.num_actions = num_actions\n",
        "\n",
        "\t\tself.eps = 1.0  # for epsilon greedy algorithm\n",
        "\t\tself.gamma = gamma\n",
        "\t\tself.r = r\n",
        "\t\tself.lr = lr\n",
        "\n",
        "\t\tself.q_table = np.random.rand(np.prod(list_state_reso), num_actions)\n",
        "\n",
        "\tdef bins(self, clip_min, clip_max, num):\n",
        "\t\treturn np.linspace(clip_min, clip_max, num + 1)[1:-1]\n",
        " \n",
        "\tdef getStateIndex(self, observation):\n",
        "\t\tlist_index = []\n",
        "\t\tfor i in range(self.num_states):\n",
        "\t\t\tindex = np.digitize(observation[i], bins=self.bins(self.list_state_range[i][0], self.list_state_range[i][1], self.list_state_reso[i]))\n",
        "\t\t\tlist_index.append(index)\n",
        "\t\treturn sum([index*int(np.prod(self.list_state_reso[:i])) for i, index in enumerate(list_index)])\n",
        "\n",
        "\tdef updateQtable(self, obs, action, reward, next_obs):\n",
        "\t\tq = self.q_table[self.getStateIndex(obs), action]\n",
        "\t\tnext_q_max = np.max(self.q_table[self.getStateIndex(next_obs)])\n",
        "\t\tself.q_table[self.getStateIndex(obs), action] = q + self.lr*(reward + self.gamma*next_q_max - q)\n",
        "\n",
        "\tdef getAction(self, obs, is_training):\n",
        "\t\tif is_training and np.random.rand() < self.eps:\n",
        "\t\t\taction = np.random.randint(self.num_actions)\n",
        "\t\telse:\n",
        "\t\t\taction = np.argmax(self.q_table[self.getStateIndex(obs)])\n",
        "\t\t## update eps\n",
        "\t\tif is_training and self.eps > 0.1:\n",
        "\t\t\tself.eps *= self.r\n",
        "\t\treturn action"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSn2nGYqzoqh"
      },
      "source": [
        "Agent class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnTqsw2uzsXP"
      },
      "source": [
        "class Agent:\n",
        "\tdef __init__(self, num_states, list_state_range, list_state_reso, num_actions, gamma, r, lr):\n",
        "\t\tself.brain = Brain(num_states, list_state_range, list_state_reso, num_actions, gamma, r, lr)\n",
        " \n",
        "\tdef updateQtable(self, obs, action, reward, next_obs):\n",
        "\t\tself.brain.updateQtable(obs, action, reward, next_obs)\n",
        " \n",
        "\tdef getAction(self, obs, is_training):\n",
        "\t\taction = self.brain.getAction(obs, is_training)\n",
        "\t\treturn action"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzJjzN5VB-pQ"
      },
      "source": [
        "Environment class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuXmrqq7CBog"
      },
      "source": [
        "class Environment:\n",
        "\tdef __init__(self, num_episodes, max_step, gamma, r, lr):\n",
        "\t\t## parameters\n",
        "\t\tself.num_episodes = num_episodes\n",
        "\t\tself.max_step = max_step\n",
        "\t\t## environment\n",
        "\t\tself.env = Monitor(gym.make('CartPole-v0'), './videos/', video_callable=(lambda ep: ep % 100 == 0), force=True)\n",
        "\t\t## agent\n",
        "\t\tnum_states = self.env.observation_space.shape[0]\t# position, velocity, angle, angular velocity\n",
        "\t\tlist_state_range = []\n",
        "\t\tfor i in range(num_states):\n",
        "\t\t\tlist_state_range.append([self.env.observation_space.low[i], self.env.observation_space.high[i]])\n",
        "\t\tlist_state_range[1] = [-3.0, 3.0]\n",
        "\t\tlist_state_range[3] = [-0.5, 0.5]\n",
        "\t\tprint(\"list_state_range = \", list_state_range)\n",
        "\t\tlist_state_reso = [4, 4, 6, 6]\n",
        "\t\tnum_actions = self.env.action_space.n\n",
        "\n",
        "\t\tself.agent = Agent(num_states, list_state_range, list_state_reso, num_actions, gamma, r, lr)\n",
        " \n",
        "\tdef train(self):\n",
        "\t\tnum_completed_episodes = 0\n",
        "  \n",
        "\t\tfor episode in range(self.num_episodes):\n",
        "\t\t\tobs = self.env.reset()\n",
        "\t\t\tepisode_reward = 0\n",
        " \n",
        "\t\t\tfor step in range(self.max_step):\n",
        "\t\t\t\t## get action\n",
        "\t\t\t\taction = self.agent.getAction(obs, is_training=True)\n",
        "\t\t\t\t## observe next step\n",
        "\t\t\t\tnext_obs, _, is_done, _ = self.env.step(action)\n",
        "\t\t\t\t## get reward\n",
        "\t\t\t\tif is_done:\n",
        "\t\t\t\t\tif step < max_step - 1:\n",
        "\t\t\t\t\t\treward = -100\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\treward = 1\n",
        "\t\t\t\t\t\tnum_completed_episodes += 1\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\treward = 1\n",
        "\t\t\t\tepisode_reward += reward\n",
        "\t\t\t\t## update\n",
        "\t\t\t\tself.agent.updateQtable(obs, action, reward, next_obs)\n",
        "\t\t\t\t## to next step\n",
        "\t\t\t\tobs = next_obs\n",
        "\n",
        "\t\t\t\tif is_done:\n",
        "\t\t\t\t\tprint('{0} Episode: Finished after {1} time steps with reward {2}'.format(episode, step+1, episode_reward))\n",
        "\t\t\t\t\tbreak\n",
        "\t\tprint(\"num_completed_episodes = \", num_completed_episodes)\n",
        "\n",
        "\tdef evaluate(self):\n",
        "\t\tobs = self.env.reset()\n",
        "\t\t\n",
        "\t\tfor step in range(self.max_step):\n",
        "\t\t\t## get action\n",
        "\t\t\taction = self.agent.getAction(obs, is_training=False)\n",
        "\t\t\t## observe next step\n",
        "\t\t\tnext_obs, _, is_done, _ = self.env.step(action)\n",
        "\t\t\t## to next step\n",
        "\t\t\tobs = next_obs\n",
        "\n",
        "\t\t\tif is_done:\n",
        "\t\t\t\tprint('Evaluation: Finished after {} time steps'.format(step+1))\n",
        "\t\t\t\tbreak"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDKojCejb2-A"
      },
      "source": [
        "Prepare showing videos of the restults"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvmszieobbrX"
      },
      "source": [
        "def show_video(env):\n",
        "\tenv.reset()\n",
        "\tfor frame in env.videos:\n",
        "\t\tprint(\"frame = \", frame)\n",
        "\t\tvideo = io.open(frame[0], 'r+b').read()\n",
        "\t\tencoded = base64.b64encode(video)\n",
        "\n",
        "\t\tdisplay.display(display.HTML(data=\"\"\"\n",
        "\t\t\t<video alt=\"\" controls>\n",
        "\t\t\t<source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "\t\t\t</video>\n",
        "\t\t\t\"\"\".format(encoded.decode('ascii')))\n",
        "\t\t)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJqfOQ-tW0sA",
        "outputId": "a3985873-a6f7-4c63-d6b2-c326066aa70f"
      },
      "source": [
        "## display\n",
        "virtual_display = Display()\n",
        "virtual_display.start()\n",
        "\n",
        "## parameters\n",
        "num_episodes = 500\n",
        "max_step = 200\n",
        "gamma = 0.9\n",
        "r = 0.99\n",
        "lr = 0.5\n",
        "\n",
        "## run\n",
        "cartpole_env = Environment(num_episodes, max_step, gamma, r, lr)\n",
        "cartpole_env.train()\n",
        "cartpole_env.evaluate()\n",
        "show_video(cartpole_env.env)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "list_state_range =  [[-4.8, 4.8], [-3.0, 3.0], [-0.41887903, 0.41887903], [-0.5, 0.5]]\n",
            "0 Episode: Finished after 17 time steps with reward -84\n",
            "1 Episode: Finished after 21 time steps with reward -80\n",
            "2 Episode: Finished after 16 time steps with reward -85\n",
            "3 Episode: Finished after 45 time steps with reward -56\n",
            "4 Episode: Finished after 29 time steps with reward -72\n",
            "5 Episode: Finished after 28 time steps with reward -73\n",
            "6 Episode: Finished after 18 time steps with reward -83\n",
            "7 Episode: Finished after 30 time steps with reward -71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mESpPTF72xIr"
      },
      "source": [
        "## References\n",
        "- [minnano_rl/section_2/01_simple_reinforcement_learning.ipynb](https://github.com/yukinaga/minnano_rl/blob/main/section_2/01_simple_reinforcement_learning.ipynb)\n",
        "- [第10回　CartPole課題をQ学習で制御する](https://book.mynavi.jp/manatee/detail/id=88997)"
      ]
    }
  ]
}