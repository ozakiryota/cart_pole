{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cartpole_dqn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPOCP5aovmejIxDVMfbN/rL",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ozakiryota/cart_pole/blob/main/cartpole_dqn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzGR6DXx5hxW"
      },
      "source": [
        "# Cart-Pole\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NztiGF0Ld3ET"
      },
      "source": [
        "Install OpenAI Gym (https://gym.openai.com)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTnd9XdOQ0JA",
        "outputId": "5bfb1a93-a341-4b00-adf5-b668698f066a"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WatqsT0I5lxF"
      },
      "source": [
        "Install the packages for visualizing Gym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uo7Lrbp5Sqod",
        "outputId": "9f4fedbd-2164-42fc-d2fb-227d33471365"
      },
      "source": [
        "!apt update\n",
        "!apt install xvfb\n",
        "!pip install pyvirtualdisplay"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [Connecting to security.u\u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [Connecting to security.u\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\u001b[0m\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\u001b[0m\r                                                                               \rGet:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r                                                                               \rGet:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [3 InRelease 14.2 kB/88.7 k\u001b[0m\r                                                                               \rHit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [3 InRelease 14.2 kB/88.7 kB 16%] [4 InRelease 2,\u001b[0m\r                                                                               \rIgn:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [3 InRelease 14.2 kB/88.7 k\u001b[0m\r                                                                               \rGet:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [3 InRelease 14.2 kB/88.7 k\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [3 InRelease 14.2 kB/88.7 k\u001b[0m\r                                                                               \rHit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [3 InRelease 14.2 kB/88.7 k\u001b[0m\r                                                                               \rGet:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Ign:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [741 kB]\n",
            "Get:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,756 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [24.7 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [396 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,116 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [31.6 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [426 kB]\n",
            "Get:23 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [899 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,546 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,181 kB]\n",
            "Fetched 11.4 MB in 3s (4,380 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "36 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 36 not upgraded.\n",
            "Need to get 784 kB of archives.\n",
            "After this operation, 2,270 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.9 [784 kB]\n",
            "Fetched 784 kB in 1s (1,268 kB/s)\n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 160690 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.9_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading https://files.pythonhosted.org/packages/19/88/7a198a5ee3baa3d547f5a49574cd8c3913b216f5276b690b028f89ffb325/PyVirtualDisplay-2.1-py3-none-any.whl\n",
            "Collecting EasyProcess\n",
            "  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
            "Successfully installed EasyProcess-0.3 pyvirtualdisplay-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cF70T7rbdLXM"
      },
      "source": [
        "## DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9MoZlOvenEx"
      },
      "source": [
        "Import Gym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDrypO0FZ9IG"
      },
      "source": [
        "import gym"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6r-cLs8ej61"
      },
      "source": [
        "Import the packages for visualizing Gym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8zm1ycgaE8K"
      },
      "source": [
        "import base64\n",
        "import io\n",
        "from gym.wrappers import Monitor\n",
        "from IPython import display\n",
        "from pyvirtualdisplay import Display"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgvZ1f4MccCJ"
      },
      "source": [
        "Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a3R-TE_gaS1"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSkAwHBHGqxK"
      },
      "source": [
        "Net class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5x31HxMGqTk"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, num_states, dim_mid, num_actions):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(num_states, dim_mid),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dim_mid, dim_mid),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dim_mid, num_actions)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Htu4VPF8cflL"
      },
      "source": [
        "Brain class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMFWoAE1RIRe"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self, num_states, num_actions, gamma, r, lr):\n",
        "        self.num_states = num_states\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(\"self.device = \", self.device)\n",
        "        self.net = Net(num_states, 64, num_actions)\n",
        "        self.net.to(self.device)\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.optimizer = optim.RMSprop(self.net.parameters(), lr=lr)\n",
        "        #self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "\n",
        "        self.eps = 1.0  # for epsilon greedy algorithm\n",
        "        self.gamma = gamma\n",
        "        self.r = r\n",
        " \n",
        "    def updateQnet(self, obs_numpy, action, reward, next_obs_numpy):\n",
        "        obs_tensor = torch.from_numpy(obs_numpy).float()\n",
        "        obs_tensor.unsqueeze_(0)\n",
        "        obs_tensor = obs_tensor.to(self.device)\n",
        "\n",
        "        next_obs_tensor = torch.from_numpy(next_obs_numpy).float()\n",
        "        next_obs_tensor.unsqueeze_(0)\n",
        "        next_obs_tensor = next_obs_tensor.to(self.device)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        self.net.train()\n",
        "        q = self.net(obs_tensor)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            self.net.eval()\n",
        "            labels = self.net(obs_tensor)\n",
        "            next_q = self.net(next_obs_tensor)\n",
        "\n",
        "            labels[:, action] = reward + self.gamma*np.max(next_q.cpu().detach().numpy(), axis=1)[0]\n",
        "        \n",
        "        loss = self.criterion(q, labels)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def getAction(self, obs_numpy, is_training):\n",
        "        if is_training and np.random.rand() < self.eps:\n",
        "            action = np.random.randint(self.num_actions)\n",
        "        else:\n",
        "            obs_tensor = torch.from_numpy(obs_numpy).float()\n",
        "            obs_tensor.unsqueeze_(0)\n",
        "            obs_tensor = obs_tensor.to(self.device)\n",
        "            with torch.no_grad():\n",
        "                self.net.eval()\n",
        "                q = self.net(obs_tensor)\n",
        "                action = np.argmax(q.cpu().detach().numpy(), axis=1)[0]\n",
        "        ## update eps\n",
        "        if is_training and self.eps > 0.1:\n",
        "            self.eps *= self.r\n",
        "        return action"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSn2nGYqzoqh"
      },
      "source": [
        "Agent class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnTqsw2uzsXP"
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, num_states, num_actions, gamma, r, lr):\n",
        "        self.brain = Brain(num_states, num_actions, gamma, r, lr)\n",
        " \n",
        "    def updateQnet(self, obs, action, reward, next_obs):\n",
        "        self.brain.updateQnet(obs, action, reward, next_obs)\n",
        " \n",
        "    def getAction(self, obs, is_training):\n",
        "        action = self.brain.getAction(obs, is_training)\n",
        "        return action"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzJjzN5VB-pQ"
      },
      "source": [
        "Environment class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuXmrqq7CBog"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, num_episodes, max_step, gamma, r, lr):\n",
        "        ## parameters\n",
        "        self.num_episodes = num_episodes\n",
        "        self.max_step = max_step\n",
        "        ## environment\n",
        "        self.env = Monitor(gym.make('CartPole-v0'), './videos/', force=True)\n",
        "        ## agent\n",
        "        num_states = self.env.observation_space.shape[0]    # position, velocity, angle, angular velocity\n",
        "        num_actions = self.env.action_space.n\n",
        "        self.agent = Agent(num_states, num_actions, gamma, r, lr)\n",
        "\n",
        "    def train(self):\n",
        "        num_completed_episodes = 0\n",
        "        \n",
        "        for episode in range(self.num_episodes):\n",
        "            obs = self.env.reset()\n",
        "            episode_reward = 0\n",
        " \n",
        "            for step in range(self.max_step):\n",
        "                ## get action\n",
        "                action = self.agent.getAction(obs, is_training=True)\n",
        "                ## observe next step\n",
        "                next_obs, _, is_done, _ = self.env.step(action)\n",
        "                ## get reward\n",
        "                if is_done:\n",
        "                    if step < max_step - 1:\n",
        "                        reward = -100\n",
        "                    else:\n",
        "                        reward = 1\n",
        "                        num_completed_episodes += 1\n",
        "                else:\n",
        "                    reward = 1\n",
        "                episode_reward += reward\n",
        "                ## update\n",
        "                self.agent.updateQnet(obs, action, reward, next_obs)\n",
        "                ## to next step\n",
        "                obs = next_obs\n",
        "\n",
        "                if is_done:\n",
        "                    print('{0} Episode: Finished after {1} time steps with reward {2}'.format(episode, step+1, episode_reward))\n",
        "                    break\n",
        "        print(\"num_completed_episodes = \", num_completed_episodes)\n",
        "\n",
        "    def evaluate(self):\n",
        "        obs = self.env.reset()\n",
        "        \n",
        "        for step in range(self.max_step):\n",
        "            ## get action\n",
        "            action = self.agent.getAction(obs, is_training=False)\n",
        "            ## observe next step\n",
        "            next_obs, _, is_done, _ = self.env.step(action)\n",
        "            ## to next step\n",
        "            obs = next_obs\n",
        "\n",
        "            if is_done:\n",
        "                print('Evaluation: Finished after {} time steps'.format(step+1))\n",
        "                break"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDKojCejb2-A"
      },
      "source": [
        "Prepare showing videos of the restults"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvmszieobbrX"
      },
      "source": [
        "def show_video(env):\n",
        "    env.reset()\n",
        "    for frame in env.videos:\n",
        "        print(\"frame = \", frame)\n",
        "        video = io.open(frame[0], 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "\n",
        "        display.display(display.HTML(data=\"\"\"\n",
        "            <video alt=\"test\" controls>\n",
        "            <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "            </video>\n",
        "            \"\"\".format(encoded.decode('ascii')))\n",
        "        )"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C7zmRMnW1i8"
      },
      "source": [
        "Run training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJqfOQ-tW0sA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "348012ed-a0e8-42bd-f20f-2b13b8129edb"
      },
      "source": [
        "## display\n",
        "virtual_display = Display()\n",
        "virtual_display.start()\n",
        "\n",
        "## parameters\n",
        "num_episodes = 1000\n",
        "max_step = 200\n",
        "gamma = 0.9\n",
        "r = 0.99\n",
        "lr = 0.001\n",
        "\n",
        "## run\n",
        "cartpole_env = Environment(num_episodes, max_step, gamma, r, lr)\n",
        "cartpole_env.train()\n",
        "cartpole_env.evaluate()\n",
        "show_video(cartpole_env.env)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "self.device =  cuda:0\n",
            "0 Episode: Finished after 18 time steps with reward -83\n",
            "1 Episode: Finished after 20 time steps with reward -81\n",
            "2 Episode: Finished after 21 time steps with reward -80\n",
            "3 Episode: Finished after 10 time steps with reward -91\n",
            "4 Episode: Finished after 11 time steps with reward -90\n",
            "5 Episode: Finished after 10 time steps with reward -91\n",
            "6 Episode: Finished after 9 time steps with reward -92\n",
            "7 Episode: Finished after 11 time steps with reward -90\n",
            "8 Episode: Finished after 13 time steps with reward -88\n",
            "9 Episode: Finished after 14 time steps with reward -87\n",
            "10 Episode: Finished after 11 time steps with reward -90\n",
            "11 Episode: Finished after 12 time steps with reward -89\n",
            "12 Episode: Finished after 13 time steps with reward -88\n",
            "13 Episode: Finished after 10 time steps with reward -91\n",
            "14 Episode: Finished after 15 time steps with reward -86\n",
            "15 Episode: Finished after 27 time steps with reward -74\n",
            "16 Episode: Finished after 13 time steps with reward -88\n",
            "17 Episode: Finished after 10 time steps with reward -91\n",
            "18 Episode: Finished after 10 time steps with reward -91\n",
            "19 Episode: Finished after 13 time steps with reward -88\n",
            "20 Episode: Finished after 9 time steps with reward -92\n",
            "21 Episode: Finished after 12 time steps with reward -89\n",
            "22 Episode: Finished after 11 time steps with reward -90\n",
            "23 Episode: Finished after 10 time steps with reward -91\n",
            "24 Episode: Finished after 11 time steps with reward -90\n",
            "25 Episode: Finished after 10 time steps with reward -91\n",
            "26 Episode: Finished after 9 time steps with reward -92\n",
            "27 Episode: Finished after 11 time steps with reward -90\n",
            "28 Episode: Finished after 9 time steps with reward -92\n",
            "29 Episode: Finished after 10 time steps with reward -91\n",
            "30 Episode: Finished after 10 time steps with reward -91\n",
            "31 Episode: Finished after 9 time steps with reward -92\n",
            "32 Episode: Finished after 11 time steps with reward -90\n",
            "33 Episode: Finished after 10 time steps with reward -91\n",
            "34 Episode: Finished after 10 time steps with reward -91\n",
            "35 Episode: Finished after 13 time steps with reward -88\n",
            "36 Episode: Finished after 11 time steps with reward -90\n",
            "37 Episode: Finished after 15 time steps with reward -86\n",
            "38 Episode: Finished after 14 time steps with reward -87\n",
            "39 Episode: Finished after 13 time steps with reward -88\n",
            "40 Episode: Finished after 15 time steps with reward -86\n",
            "41 Episode: Finished after 12 time steps with reward -89\n",
            "42 Episode: Finished after 17 time steps with reward -84\n",
            "43 Episode: Finished after 18 time steps with reward -83\n",
            "44 Episode: Finished after 20 time steps with reward -81\n",
            "45 Episode: Finished after 20 time steps with reward -81\n",
            "46 Episode: Finished after 13 time steps with reward -88\n",
            "47 Episode: Finished after 16 time steps with reward -85\n",
            "48 Episode: Finished after 13 time steps with reward -88\n",
            "49 Episode: Finished after 14 time steps with reward -87\n",
            "50 Episode: Finished after 16 time steps with reward -85\n",
            "51 Episode: Finished after 15 time steps with reward -86\n",
            "52 Episode: Finished after 16 time steps with reward -85\n",
            "53 Episode: Finished after 20 time steps with reward -81\n",
            "54 Episode: Finished after 19 time steps with reward -82\n",
            "55 Episode: Finished after 18 time steps with reward -83\n",
            "56 Episode: Finished after 17 time steps with reward -84\n",
            "57 Episode: Finished after 16 time steps with reward -85\n",
            "58 Episode: Finished after 12 time steps with reward -89\n",
            "59 Episode: Finished after 15 time steps with reward -86\n",
            "60 Episode: Finished after 14 time steps with reward -87\n",
            "61 Episode: Finished after 13 time steps with reward -88\n",
            "62 Episode: Finished after 16 time steps with reward -85\n",
            "63 Episode: Finished after 15 time steps with reward -86\n",
            "64 Episode: Finished after 10 time steps with reward -91\n",
            "65 Episode: Finished after 18 time steps with reward -83\n",
            "66 Episode: Finished after 12 time steps with reward -89\n",
            "67 Episode: Finished after 18 time steps with reward -83\n",
            "68 Episode: Finished after 17 time steps with reward -84\n",
            "69 Episode: Finished after 23 time steps with reward -78\n",
            "70 Episode: Finished after 20 time steps with reward -81\n",
            "71 Episode: Finished after 17 time steps with reward -84\n",
            "72 Episode: Finished after 22 time steps with reward -79\n",
            "73 Episode: Finished after 30 time steps with reward -71\n",
            "74 Episode: Finished after 10 time steps with reward -91\n",
            "75 Episode: Finished after 10 time steps with reward -91\n",
            "76 Episode: Finished after 11 time steps with reward -90\n",
            "77 Episode: Finished after 8 time steps with reward -93\n",
            "78 Episode: Finished after 9 time steps with reward -92\n",
            "79 Episode: Finished after 12 time steps with reward -89\n",
            "80 Episode: Finished after 10 time steps with reward -91\n",
            "81 Episode: Finished after 10 time steps with reward -91\n",
            "82 Episode: Finished after 11 time steps with reward -90\n",
            "83 Episode: Finished after 10 time steps with reward -91\n",
            "84 Episode: Finished after 9 time steps with reward -92\n",
            "85 Episode: Finished after 9 time steps with reward -92\n",
            "86 Episode: Finished after 23 time steps with reward -78\n",
            "87 Episode: Finished after 18 time steps with reward -83\n",
            "88 Episode: Finished after 16 time steps with reward -85\n",
            "89 Episode: Finished after 15 time steps with reward -86\n",
            "90 Episode: Finished after 17 time steps with reward -84\n",
            "91 Episode: Finished after 20 time steps with reward -81\n",
            "92 Episode: Finished after 26 time steps with reward -75\n",
            "93 Episode: Finished after 33 time steps with reward -68\n",
            "94 Episode: Finished after 10 time steps with reward -91\n",
            "95 Episode: Finished after 11 time steps with reward -90\n",
            "96 Episode: Finished after 47 time steps with reward -54\n",
            "97 Episode: Finished after 18 time steps with reward -83\n",
            "98 Episode: Finished after 23 time steps with reward -78\n",
            "99 Episode: Finished after 26 time steps with reward -75\n",
            "100 Episode: Finished after 65 time steps with reward -36\n",
            "101 Episode: Finished after 11 time steps with reward -90\n",
            "102 Episode: Finished after 71 time steps with reward -30\n",
            "103 Episode: Finished after 19 time steps with reward -82\n",
            "104 Episode: Finished after 55 time steps with reward -46\n",
            "105 Episode: Finished after 27 time steps with reward -74\n",
            "106 Episode: Finished after 105 time steps with reward 4\n",
            "107 Episode: Finished after 29 time steps with reward -72\n",
            "108 Episode: Finished after 16 time steps with reward -85\n",
            "109 Episode: Finished after 15 time steps with reward -86\n",
            "110 Episode: Finished after 17 time steps with reward -84\n",
            "111 Episode: Finished after 11 time steps with reward -90\n",
            "112 Episode: Finished after 12 time steps with reward -89\n",
            "113 Episode: Finished after 11 time steps with reward -90\n",
            "114 Episode: Finished after 20 time steps with reward -81\n",
            "115 Episode: Finished after 34 time steps with reward -67\n",
            "116 Episode: Finished after 22 time steps with reward -79\n",
            "117 Episode: Finished after 15 time steps with reward -86\n",
            "118 Episode: Finished after 36 time steps with reward -65\n",
            "119 Episode: Finished after 22 time steps with reward -79\n",
            "120 Episode: Finished after 14 time steps with reward -87\n",
            "121 Episode: Finished after 14 time steps with reward -87\n",
            "122 Episode: Finished after 11 time steps with reward -90\n",
            "123 Episode: Finished after 10 time steps with reward -91\n",
            "124 Episode: Finished after 9 time steps with reward -92\n",
            "125 Episode: Finished after 13 time steps with reward -88\n",
            "126 Episode: Finished after 16 time steps with reward -85\n",
            "127 Episode: Finished after 12 time steps with reward -89\n",
            "128 Episode: Finished after 13 time steps with reward -88\n",
            "129 Episode: Finished after 14 time steps with reward -87\n",
            "130 Episode: Finished after 12 time steps with reward -89\n",
            "131 Episode: Finished after 10 time steps with reward -91\n",
            "132 Episode: Finished after 10 time steps with reward -91\n",
            "133 Episode: Finished after 10 time steps with reward -91\n",
            "134 Episode: Finished after 10 time steps with reward -91\n",
            "135 Episode: Finished after 12 time steps with reward -89\n",
            "136 Episode: Finished after 12 time steps with reward -89\n",
            "137 Episode: Finished after 13 time steps with reward -88\n",
            "138 Episode: Finished after 17 time steps with reward -84\n",
            "139 Episode: Finished after 14 time steps with reward -87\n",
            "140 Episode: Finished after 10 time steps with reward -91\n",
            "141 Episode: Finished after 11 time steps with reward -90\n",
            "142 Episode: Finished after 10 time steps with reward -91\n",
            "143 Episode: Finished after 19 time steps with reward -82\n",
            "144 Episode: Finished after 11 time steps with reward -90\n",
            "145 Episode: Finished after 11 time steps with reward -90\n",
            "146 Episode: Finished after 11 time steps with reward -90\n",
            "147 Episode: Finished after 14 time steps with reward -87\n",
            "148 Episode: Finished after 18 time steps with reward -83\n",
            "149 Episode: Finished after 25 time steps with reward -76\n",
            "150 Episode: Finished after 16 time steps with reward -85\n",
            "151 Episode: Finished after 16 time steps with reward -85\n",
            "152 Episode: Finished after 9 time steps with reward -92\n",
            "153 Episode: Finished after 10 time steps with reward -91\n",
            "154 Episode: Finished after 12 time steps with reward -89\n",
            "155 Episode: Finished after 9 time steps with reward -92\n",
            "156 Episode: Finished after 13 time steps with reward -88\n",
            "157 Episode: Finished after 16 time steps with reward -85\n",
            "158 Episode: Finished after 15 time steps with reward -86\n",
            "159 Episode: Finished after 10 time steps with reward -91\n",
            "160 Episode: Finished after 10 time steps with reward -91\n",
            "161 Episode: Finished after 12 time steps with reward -89\n",
            "162 Episode: Finished after 10 time steps with reward -91\n",
            "163 Episode: Finished after 8 time steps with reward -93\n",
            "164 Episode: Finished after 9 time steps with reward -92\n",
            "165 Episode: Finished after 17 time steps with reward -84\n",
            "166 Episode: Finished after 12 time steps with reward -89\n",
            "167 Episode: Finished after 18 time steps with reward -83\n",
            "168 Episode: Finished after 12 time steps with reward -89\n",
            "169 Episode: Finished after 15 time steps with reward -86\n",
            "170 Episode: Finished after 29 time steps with reward -72\n",
            "171 Episode: Finished after 32 time steps with reward -69\n",
            "172 Episode: Finished after 25 time steps with reward -76\n",
            "173 Episode: Finished after 17 time steps with reward -84\n",
            "174 Episode: Finished after 40 time steps with reward -61\n",
            "175 Episode: Finished after 33 time steps with reward -68\n",
            "176 Episode: Finished after 18 time steps with reward -83\n",
            "177 Episode: Finished after 62 time steps with reward -39\n",
            "178 Episode: Finished after 48 time steps with reward -53\n",
            "179 Episode: Finished after 69 time steps with reward -32\n",
            "180 Episode: Finished after 86 time steps with reward -15\n",
            "181 Episode: Finished after 10 time steps with reward -91\n",
            "182 Episode: Finished after 40 time steps with reward -61\n",
            "183 Episode: Finished after 21 time steps with reward -80\n",
            "184 Episode: Finished after 19 time steps with reward -82\n",
            "185 Episode: Finished after 10 time steps with reward -91\n",
            "186 Episode: Finished after 42 time steps with reward -59\n",
            "187 Episode: Finished after 53 time steps with reward -48\n",
            "188 Episode: Finished after 39 time steps with reward -62\n",
            "189 Episode: Finished after 20 time steps with reward -81\n",
            "190 Episode: Finished after 10 time steps with reward -91\n",
            "191 Episode: Finished after 74 time steps with reward -27\n",
            "192 Episode: Finished after 177 time steps with reward 76\n",
            "193 Episode: Finished after 8 time steps with reward -93\n",
            "194 Episode: Finished after 9 time steps with reward -92\n",
            "195 Episode: Finished after 21 time steps with reward -80\n",
            "196 Episode: Finished after 10 time steps with reward -91\n",
            "197 Episode: Finished after 13 time steps with reward -88\n",
            "198 Episode: Finished after 10 time steps with reward -91\n",
            "199 Episode: Finished after 18 time steps with reward -83\n",
            "200 Episode: Finished after 13 time steps with reward -88\n",
            "201 Episode: Finished after 24 time steps with reward -77\n",
            "202 Episode: Finished after 10 time steps with reward -91\n",
            "203 Episode: Finished after 10 time steps with reward -91\n",
            "204 Episode: Finished after 9 time steps with reward -92\n",
            "205 Episode: Finished after 11 time steps with reward -90\n",
            "206 Episode: Finished after 10 time steps with reward -91\n",
            "207 Episode: Finished after 10 time steps with reward -91\n",
            "208 Episode: Finished after 11 time steps with reward -90\n",
            "209 Episode: Finished after 9 time steps with reward -92\n",
            "210 Episode: Finished after 10 time steps with reward -91\n",
            "211 Episode: Finished after 9 time steps with reward -92\n",
            "212 Episode: Finished after 9 time steps with reward -92\n",
            "213 Episode: Finished after 9 time steps with reward -92\n",
            "214 Episode: Finished after 9 time steps with reward -92\n",
            "215 Episode: Finished after 9 time steps with reward -92\n",
            "216 Episode: Finished after 9 time steps with reward -92\n",
            "217 Episode: Finished after 12 time steps with reward -89\n",
            "218 Episode: Finished after 9 time steps with reward -92\n",
            "219 Episode: Finished after 11 time steps with reward -90\n",
            "220 Episode: Finished after 10 time steps with reward -91\n",
            "221 Episode: Finished after 10 time steps with reward -91\n",
            "222 Episode: Finished after 10 time steps with reward -91\n",
            "223 Episode: Finished after 9 time steps with reward -92\n",
            "224 Episode: Finished after 13 time steps with reward -88\n",
            "225 Episode: Finished after 9 time steps with reward -92\n",
            "226 Episode: Finished after 12 time steps with reward -89\n",
            "227 Episode: Finished after 10 time steps with reward -91\n",
            "228 Episode: Finished after 11 time steps with reward -90\n",
            "229 Episode: Finished after 8 time steps with reward -93\n",
            "230 Episode: Finished after 20 time steps with reward -81\n",
            "231 Episode: Finished after 10 time steps with reward -91\n",
            "232 Episode: Finished after 10 time steps with reward -91\n",
            "233 Episode: Finished after 11 time steps with reward -90\n",
            "234 Episode: Finished after 10 time steps with reward -91\n",
            "235 Episode: Finished after 10 time steps with reward -91\n",
            "236 Episode: Finished after 10 time steps with reward -91\n",
            "237 Episode: Finished after 12 time steps with reward -89\n",
            "238 Episode: Finished after 8 time steps with reward -93\n",
            "239 Episode: Finished after 10 time steps with reward -91\n",
            "240 Episode: Finished after 23 time steps with reward -78\n",
            "241 Episode: Finished after 20 time steps with reward -81\n",
            "242 Episode: Finished after 16 time steps with reward -85\n",
            "243 Episode: Finished after 9 time steps with reward -92\n",
            "244 Episode: Finished after 13 time steps with reward -88\n",
            "245 Episode: Finished after 38 time steps with reward -63\n",
            "246 Episode: Finished after 40 time steps with reward -61\n",
            "247 Episode: Finished after 18 time steps with reward -83\n",
            "248 Episode: Finished after 8 time steps with reward -93\n",
            "249 Episode: Finished after 24 time steps with reward -77\n",
            "250 Episode: Finished after 96 time steps with reward -5\n",
            "251 Episode: Finished after 34 time steps with reward -67\n",
            "252 Episode: Finished after 11 time steps with reward -90\n",
            "253 Episode: Finished after 22 time steps with reward -79\n",
            "254 Episode: Finished after 11 time steps with reward -90\n",
            "255 Episode: Finished after 17 time steps with reward -84\n",
            "256 Episode: Finished after 11 time steps with reward -90\n",
            "257 Episode: Finished after 8 time steps with reward -93\n",
            "258 Episode: Finished after 12 time steps with reward -89\n",
            "259 Episode: Finished after 9 time steps with reward -92\n",
            "260 Episode: Finished after 11 time steps with reward -90\n",
            "261 Episode: Finished after 9 time steps with reward -92\n",
            "262 Episode: Finished after 14 time steps with reward -87\n",
            "263 Episode: Finished after 10 time steps with reward -91\n",
            "264 Episode: Finished after 10 time steps with reward -91\n",
            "265 Episode: Finished after 9 time steps with reward -92\n",
            "266 Episode: Finished after 11 time steps with reward -90\n",
            "267 Episode: Finished after 20 time steps with reward -81\n",
            "268 Episode: Finished after 16 time steps with reward -85\n",
            "269 Episode: Finished after 11 time steps with reward -90\n",
            "270 Episode: Finished after 12 time steps with reward -89\n",
            "271 Episode: Finished after 9 time steps with reward -92\n",
            "272 Episode: Finished after 10 time steps with reward -91\n",
            "273 Episode: Finished after 14 time steps with reward -87\n",
            "274 Episode: Finished after 10 time steps with reward -91\n",
            "275 Episode: Finished after 24 time steps with reward -77\n",
            "276 Episode: Finished after 13 time steps with reward -88\n",
            "277 Episode: Finished after 32 time steps with reward -69\n",
            "278 Episode: Finished after 21 time steps with reward -80\n",
            "279 Episode: Finished after 12 time steps with reward -89\n",
            "280 Episode: Finished after 8 time steps with reward -93\n",
            "281 Episode: Finished after 17 time steps with reward -84\n",
            "282 Episode: Finished after 13 time steps with reward -88\n",
            "283 Episode: Finished after 12 time steps with reward -89\n",
            "284 Episode: Finished after 23 time steps with reward -78\n",
            "285 Episode: Finished after 31 time steps with reward -70\n",
            "286 Episode: Finished after 33 time steps with reward -68\n",
            "287 Episode: Finished after 22 time steps with reward -79\n",
            "288 Episode: Finished after 16 time steps with reward -85\n",
            "289 Episode: Finished after 14 time steps with reward -87\n",
            "290 Episode: Finished after 10 time steps with reward -91\n",
            "291 Episode: Finished after 10 time steps with reward -91\n",
            "292 Episode: Finished after 9 time steps with reward -92\n",
            "293 Episode: Finished after 11 time steps with reward -90\n",
            "294 Episode: Finished after 9 time steps with reward -92\n",
            "295 Episode: Finished after 11 time steps with reward -90\n",
            "296 Episode: Finished after 12 time steps with reward -89\n",
            "297 Episode: Finished after 29 time steps with reward -72\n",
            "298 Episode: Finished after 38 time steps with reward -63\n",
            "299 Episode: Finished after 36 time steps with reward -65\n",
            "300 Episode: Finished after 72 time steps with reward -29\n",
            "301 Episode: Finished after 174 time steps with reward 73\n",
            "302 Episode: Finished after 29 time steps with reward -72\n",
            "303 Episode: Finished after 41 time steps with reward -60\n",
            "304 Episode: Finished after 10 time steps with reward -91\n",
            "305 Episode: Finished after 35 time steps with reward -66\n",
            "306 Episode: Finished after 71 time steps with reward -30\n",
            "307 Episode: Finished after 22 time steps with reward -79\n",
            "308 Episode: Finished after 14 time steps with reward -87\n",
            "309 Episode: Finished after 9 time steps with reward -92\n",
            "310 Episode: Finished after 16 time steps with reward -85\n",
            "311 Episode: Finished after 11 time steps with reward -90\n",
            "312 Episode: Finished after 21 time steps with reward -80\n",
            "313 Episode: Finished after 13 time steps with reward -88\n",
            "314 Episode: Finished after 10 time steps with reward -91\n",
            "315 Episode: Finished after 13 time steps with reward -88\n",
            "316 Episode: Finished after 8 time steps with reward -93\n",
            "317 Episode: Finished after 11 time steps with reward -90\n",
            "318 Episode: Finished after 9 time steps with reward -92\n",
            "319 Episode: Finished after 16 time steps with reward -85\n",
            "320 Episode: Finished after 9 time steps with reward -92\n",
            "321 Episode: Finished after 12 time steps with reward -89\n",
            "322 Episode: Finished after 10 time steps with reward -91\n",
            "323 Episode: Finished after 10 time steps with reward -91\n",
            "324 Episode: Finished after 13 time steps with reward -88\n",
            "325 Episode: Finished after 17 time steps with reward -84\n",
            "326 Episode: Finished after 18 time steps with reward -83\n",
            "327 Episode: Finished after 33 time steps with reward -68\n",
            "328 Episode: Finished after 35 time steps with reward -66\n",
            "329 Episode: Finished after 37 time steps with reward -64\n",
            "330 Episode: Finished after 42 time steps with reward -59\n",
            "331 Episode: Finished after 200 time steps with reward 200\n",
            "332 Episode: Finished after 81 time steps with reward -20\n",
            "333 Episode: Finished after 15 time steps with reward -86\n",
            "334 Episode: Finished after 10 time steps with reward -91\n",
            "335 Episode: Finished after 9 time steps with reward -92\n",
            "336 Episode: Finished after 11 time steps with reward -90\n",
            "337 Episode: Finished after 11 time steps with reward -90\n",
            "338 Episode: Finished after 9 time steps with reward -92\n",
            "339 Episode: Finished after 19 time steps with reward -82\n",
            "340 Episode: Finished after 14 time steps with reward -87\n",
            "341 Episode: Finished after 12 time steps with reward -89\n",
            "342 Episode: Finished after 17 time steps with reward -84\n",
            "343 Episode: Finished after 11 time steps with reward -90\n",
            "344 Episode: Finished after 11 time steps with reward -90\n",
            "345 Episode: Finished after 10 time steps with reward -91\n",
            "346 Episode: Finished after 11 time steps with reward -90\n",
            "347 Episode: Finished after 9 time steps with reward -92\n",
            "348 Episode: Finished after 10 time steps with reward -91\n",
            "349 Episode: Finished after 11 time steps with reward -90\n",
            "350 Episode: Finished after 9 time steps with reward -92\n",
            "351 Episode: Finished after 9 time steps with reward -92\n",
            "352 Episode: Finished after 10 time steps with reward -91\n",
            "353 Episode: Finished after 10 time steps with reward -91\n",
            "354 Episode: Finished after 10 time steps with reward -91\n",
            "355 Episode: Finished after 9 time steps with reward -92\n",
            "356 Episode: Finished after 9 time steps with reward -92\n",
            "357 Episode: Finished after 10 time steps with reward -91\n",
            "358 Episode: Finished after 12 time steps with reward -89\n",
            "359 Episode: Finished after 12 time steps with reward -89\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OreaPHjM4BJm"
      },
      "source": [
        "## References\n",
        "- [minnano_rl/section_4/02_deep_reinforcement_learning.ipynb](https://github.com/yukinaga/minnano_rl/blob/main/section_4/02_deep_reinforcement_learning.ipynb)\n",
        "- [第15回　CartPole課題で深層強化学習DQNを実装](https://book.mynavi.jp/manatee/detail/id=89831)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qvj_G-NFhkCX"
      },
      "source": [
        "## Appendix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFM6L1NEhmIa",
        "outputId": "4e4c1835-6807-4cd7-82ab-d48256c1430d"
      },
      "source": [
        "l = [\n",
        "    [\"a1\", \"a2\", \"a3\"],\n",
        "    [\"b1\", \"b2\", \"b3\"],\n",
        "    [\"c1\", \"c2\", \"c3\"],\n",
        "    [\"d1\", \"d2\", \"d3\"]\n",
        "]\n",
        "print(\"l = \", l)\n",
        "print(\"*l = \", *l)\n",
        "print(\"zip(*l) = \", zip(*l))\n",
        "print(\"*zip(*l) = \", *zip(*l))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "l =  [['a1', 'a2', 'a3'], ['b1', 'b2', 'b3'], ['c1', 'c2', 'c3'], ['d1', 'd2', 'd3']]\n",
            "*l =  ['a1', 'a2', 'a3'] ['b1', 'b2', 'b3'] ['c1', 'c2', 'c3'] ['d1', 'd2', 'd3']\n",
            "zip(*l) =  <zip object at 0x7f2436bd3870>\n",
            "*zip(*l) =  ('a1', 'b1', 'c1', 'd1') ('a2', 'b2', 'c2', 'd2') ('a3', 'b3', 'c3', 'd3')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ae5P0Fj1yUJP",
        "outputId": "323c108d-eaa5-47e7-85d8-0525d3d397ce"
      },
      "source": [
        "import torch\n",
        "\n",
        "input = torch.tensor([[1, 2, 3],[4, 5, 6],[7, 8, 9]])\n",
        "indices = torch.tensor([[0, 0, 0],[0, 1, 2],[2, 0, 1]])\n",
        "\n",
        "print(\"input = \\n\", input)\n",
        "print(\"indices = \\n\", indices)\n",
        "\n",
        "## Switch values of the tensor according to indices of row (dim=0)\n",
        "print(\"torch.gather(input=input, dim=0, index=indices) = \\n\", torch.gather(input=input, dim=0, index=indices))\n",
        "'''\n",
        "    input[0][0], input[0][1], input[0][2]\n",
        "    input[0][0], input[1][1], input[2][2]\n",
        "    input[2][0], input[0][1], input[1][2]\n",
        "'''\n",
        "\n",
        "## Switch values of the tensor according to indices of col (dim=1)\n",
        "print(\"torch.gather(input=input, dim=1, index=indices) = \\n\", torch.gather(input=input, dim=1, index=indices))\n",
        "'''\n",
        "    input[0][0], input[0][0], input[0][0]\n",
        "    input[1][0], input[1][1], input[2][2]\n",
        "    input[2][2], input[2][0], input[2][1]\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input = \n",
            " tensor([[1, 2, 3],\n",
            "        [4, 5, 6],\n",
            "        [7, 8, 9]])\n",
            "indices = \n",
            " tensor([[0, 0, 0],\n",
            "        [0, 1, 2],\n",
            "        [2, 0, 1]])\n",
            "torch.gather(input=input, dim=0, index=indices) = \n",
            " tensor([[1, 2, 3],\n",
            "        [1, 5, 9],\n",
            "        [7, 2, 6]])\n",
            "torch.gather(input=input, dim=1, index=indices) = \n",
            " tensor([[1, 1, 1],\n",
            "        [4, 5, 6],\n",
            "        [9, 7, 8]])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}