{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cart_pole.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOlAYnfz8eUygdGkjwE/3DD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ozakiryota/cart_pole/blob/main/cart_pole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzGR6DXx5hxW"
      },
      "source": [
        "# Cart-Pole\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NztiGF0Ld3ET"
      },
      "source": [
        "Install OpenAI Gym (https://gym.openai.com)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTnd9XdOQ0JA",
        "outputId": "93a1720e-1f2c-4648-a4b4-cb3ededc1f7d"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WatqsT0I5lxF"
      },
      "source": [
        "Install the packages for visualizing Gym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uo7Lrbp5Sqod",
        "outputId": "64abb910-7315-4737-d672-cbca2e595b2e"
      },
      "source": [
        "!apt update\n",
        "!apt install xvfb\n",
        "!pip install pyvirtualdisplay"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [1 InRelease 14.2 kB/88.7\u001b[0m\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [1 InRelease 14.2 kB/88.7\u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [1 InRelease 43.1 kB/88.7\u001b[0m\u001b[33m\r0% [2 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\u001b[0m\u001b[33m\r0% [2 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\u001b[0m\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\u001b[33m\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\u001b[0m\r                                                                               \rGet:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "\u001b[33m\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [4 InRelease 2,572 B/15.9 k\u001b[0m\r                                                                               \rHit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\u001b[33m\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [4 InRelease 5,468 B/15.9 k\u001b[0m\r                                                                               \rIgn:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\u001b[33m\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [4 InRelease 14.2 kB/15.9 k\u001b[0m\r                                                                               \rGet:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "\u001b[33m\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [4 InRelease 14.2 kB/15.9 k\u001b[0m\u001b[33m\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [4 InRelease 14.2 kB/15.9 k\u001b[0m\r                                                                               \rHit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "\u001b[33m\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [4 InRelease 14.2 kB/15.9 k\u001b[0m\r                                                                               \rGet:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,408 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [395 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [24.5 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,112 kB]\n",
            "Get:16 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:18 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Ign:20 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:20 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [696 kB]\n",
            "Get:21 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,755 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [425 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [31.4 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,542 kB]\n",
            "Get:25 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [898 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,179 kB]\n",
            "Get:27 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [39.5 kB]\n",
            "Fetched 12.8 MB in 3s (4,535 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "57 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  xserver-common\n",
            "Recommended packages:\n",
            "  xfonts-base\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "The following packages will be upgraded:\n",
            "  xserver-common\n",
            "1 upgraded, 1 newly installed, 0 to remove and 56 not upgraded.\n",
            "Need to get 811 kB of archives.\n",
            "After this operation, 2,270 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 xserver-common all 2:1.19.6-1ubuntu4.9 [26.8 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.9 [784 kB]\n",
            "Fetched 811 kB in 1s (1,263 kB/s)\n",
            "(Reading database ... 160983 files and directories currently installed.)\n",
            "Preparing to unpack .../xserver-common_2%3a1.19.6-1ubuntu4.9_all.deb ...\n",
            "Unpacking xserver-common (2:1.19.6-1ubuntu4.9) over (2:1.19.6-1ubuntu4.8) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.9_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Setting up xserver-common (2:1.19.6-1ubuntu4.9) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading https://files.pythonhosted.org/packages/19/88/7a198a5ee3baa3d547f5a49574cd8c3913b216f5276b690b028f89ffb325/PyVirtualDisplay-2.1-py3-none-any.whl\n",
            "Collecting EasyProcess\n",
            "  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
            "Successfully installed EasyProcess-0.3 pyvirtualdisplay-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cF70T7rbdLXM"
      },
      "source": [
        "## Random move"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9MoZlOvenEx"
      },
      "source": [
        "Import Gym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDrypO0FZ9IG"
      },
      "source": [
        "import gym"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6r-cLs8ej61"
      },
      "source": [
        "Import the packages for visualizing Gym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8zm1ycgaE8K"
      },
      "source": [
        "import base64\n",
        "import io\n",
        "from gym.wrappers import Monitor\n",
        "from IPython import display\n",
        "from pyvirtualdisplay import Display"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaDIbpNE5dEJ"
      },
      "source": [
        "Move the cart-pole randomly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0fqxHSzwRByB",
        "outputId": "9fc3ba0d-9e02-4777-d30d-45e48e39ee88"
      },
      "source": [
        "d = Display()\n",
        "d.start()\n",
        "\n",
        "env = Monitor(gym.make('CartPole-v0'),'./videos/random_move/', force=True)\n",
        "print(\"env.observation_space.shape = \", env.observation_space.shape)\n",
        "print(\"env.action_space.n = \", env.action_space.n)\n",
        "\n",
        "obs = env.reset()\n",
        "\n",
        "for t in range(100):\n",
        "    obs, reward, is_done, info = env.step(env.action_space.sample())\n",
        "    print(\"obs = \", obs)\n",
        "    print(\"reward = \", reward)\n",
        "    print(\"info = \", info)\n",
        "\n",
        "    if is_done:\n",
        "        print(\"Episode finished after {} timesteps\".format(t+1))\n",
        "        env.reset()\n",
        "        break\n",
        "\n",
        "for frame in env.videos:\n",
        "    print(\"frame = \", frame)\n",
        "    video = io.open(frame[0], 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "\n",
        "    display.display(display.HTML(data=\"\"\"\n",
        "        <video alt=\"test\" controls>\n",
        "        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "        </video>\n",
        "        \"\"\".format(encoded.decode('ascii'))))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env.observation_space.shape =  (4,)\n",
            "env.action_space.n =  2\n",
            "obs =  [ 0.01932852  0.15622655 -0.03086705 -0.35115304]\n",
            "reward =  1.0\n",
            "info =  {}\n",
            "obs =  [ 0.02245305  0.35177356 -0.03789011 -0.6534073 ]\n",
            "reward =  1.0\n",
            "info =  {}\n",
            "obs =  [ 0.02948852  0.15719914 -0.05095825 -0.37289217]\n",
            "reward =  1.0\n",
            "info =  {}\n",
            "obs =  [ 0.03263251 -0.03716327 -0.0584161  -0.09670225]\n",
            "reward =  1.0\n",
            "info =  {}\n",
            "obs =  [ 0.03188924  0.15874515 -0.06035014 -0.4072282 ]\n",
            "reward =  1.0\n",
            "info =  {}\n",
            "obs =  [ 0.03506414 -0.03547145 -0.06849471 -0.13416583]\n",
            "reward =  1.0\n",
            "info =  {}\n",
            "obs =  [ 0.03435472 -0.22954884 -0.07117802  0.13614593]\n",
            "reward =  1.0\n",
            "info =  {}\n",
            "obs =  [ 0.02976374 -0.03348338 -0.0684551  -0.17811625]\n",
            "reward =  1.0\n",
            "info =  {}\n",
            "obs =  [ 0.02909407  0.16254802 -0.07201743 -0.49158475]\n",
            "reward =  1.0\n",
            "info =  {}\n",
            "obs =  [ 0.03234503 -0.03148812 -0.08184912 -0.22243982]\n",
            "reward =  1.0\n",
            "info =  {}\n",
            "obs =  [ 0.03171527  0.16470253 -0.08629792 -0.53977737]\n",
            "reward =  1.0\n",
            "info =  {}\n",
            "obs =  [ 0.03500932 -0.02910707 -0.09709347 -0.27548494]\n",
            "reward =  1.0\n",
            "info =  {}\n",
            "obs =  [ 0.03442718  0.16725641 -0.10260317 -0.59714353]\n",
            "reward =  1.0\n",
            "info =  {}\n",
            "obs =  [ 0.03777231 -0.02629143 -0.11454604 -0.33846103]\n",
            "reward =  1.0\n",
            "info =  {}\n",
            "obs =  [ 0.03724648 -0.21961294 -0.12131526 -0.08398202]\n",
            "reward =  1.0\n",
            "info =  {}\n",
            "obs =  [ 0.03285422 -0.41280588 -0.1229949   0.16809826]\n",
            "reward =  1.0\n",
            "info =  {}\n",
            "obs =  [ 0.0245981  -0.2161577  -0.11963293 -0.16071509]\n",
            "reward =  1.0\n",
            "info =  {}\n",
            "obs =  [ 0.02027495 -0.01954428 -0.12284724 -0.48861552]\n",
            "reward =  1.0\n",
            "info =  {}\n",
            "obs =  [ 0.01988406  0.17707723 -0.13261955 -0.81735143]\n",
            "reward =  1.0\n",
            "info =  {}\n",
            "obs =  [ 0.02342561  0.37374113 -0.14896657 -1.14863283]\n",
            "reward =  1.0\n",
            "info =  {}\n",
            "obs =  [ 0.03090043  0.18084397 -0.17193923 -0.90612598]\n",
            "reward =  1.0\n",
            "info =  {}\n",
            "obs =  [ 0.03451731  0.3778245  -0.19006175 -1.24754144]\n",
            "reward =  1.0\n",
            "info =  {}\n",
            "obs =  [ 0.0420738   0.57480578 -0.21501258 -1.59323501]\n",
            "reward =  1.0\n",
            "info =  {}\n",
            "Episode finished after 23 timesteps\n",
            "frame =  ('/content/videos/random_move/openaigym.video.0.58.video000000.mp4', '/content/videos/random_move/openaigym.video.0.58.video000000.meta.json')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "        <video alt=\"test\" controls>\n",
              "        <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "        </video>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbBYRiI4Bgd-"
      },
      "source": [
        "## Q-learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgvZ1f4MccCJ"
      },
      "source": [
        "Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a3R-TE_gaS1"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Htu4VPF8cflL"
      },
      "source": [
        "Brain class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMFWoAE1RIRe"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self, num_states, list_state_range, list_state_reso, num_actions, gamma, r, lr):\n",
        "        self.num_states = num_states\n",
        "        self.list_state_range = list_state_range\n",
        "        self.list_state_reso = list_state_reso\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "        self.eps = 1.0  # for epsilon greedy algorithm\n",
        "        self.gamma = gamma\n",
        "        self.r = r\n",
        "        self.lr = lr\n",
        "\n",
        "        self.q_table = np.random.rand(np.prod(list_state_reso), num_actions)\n",
        "\n",
        "    def bins(self, clip_min, clip_max, num):\n",
        "        return np.linspace(clip_min, clip_max, num + 1)[1:-1]\n",
        " \n",
        "    def getStateIndex(self, observation):\n",
        "        list_index = []\n",
        "        for i in range(self.num_states):\n",
        "            index = np.digitize(observation[i], bins=self.bins(self.list_state_range[i][0], self.list_state_range[i][1], self.list_state_reso[i]))\n",
        "            list_index.append(index)\n",
        "        return sum([index*int(np.prod(self.list_state_reso[:i])) for i, index in enumerate(list_index)])\n",
        "\n",
        "    def updateQtable(self, obs, action, reward, next_obs):\n",
        "        q = self.q_table[self.getStateIndex(obs), action]\n",
        "        next_q_max = np.max(self.q_table[self.getStateIndex(next_obs)])\n",
        "        self.q_table[self.getStateIndex(obs), action] = q + self.lr*(reward + self.gamma*next_q_max - q)\n",
        "\n",
        "    def getAction(self, obs, is_training):\n",
        "        if is_training and np.random.rand() < self.eps:\n",
        "            action = np.random.randint(self.num_actions)\n",
        "        else:\n",
        "            action = np.argmax(self.q_table[self.getStateIndex(obs)])\n",
        "        ## update eps\n",
        "        if is_training and self.eps > 0.1:\n",
        "            self.eps *= self.r\n",
        "        return action"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSn2nGYqzoqh"
      },
      "source": [
        "Agent class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnTqsw2uzsXP"
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, num_states, list_state_range, list_state_reso, num_actions, gamma, r, lr):\n",
        "        self.brain = Brain(num_states, list_state_range, list_state_reso, num_actions, gamma, r, lr)\n",
        " \n",
        "    def updateQtable(self, obs, action, reward, next_obs):\n",
        "        self.brain.updateQtable(obs, action, reward, next_obs)\n",
        " \n",
        "    def getAction(self, obs, is_training):\n",
        "        action = self.brain.getAction(obs, is_training)\n",
        "        return action"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzJjzN5VB-pQ"
      },
      "source": [
        "Environment class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuXmrqq7CBog"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, num_episodes, max_step, gamma, r, lr):\n",
        "        ## parameters\n",
        "        self.num_episodes = num_episodes\n",
        "        self.max_step = max_step\n",
        "        ## environment\n",
        "        self.env = Monitor(gym.make('CartPole-v0'), './videos/q_learning/', force=True)\n",
        "        ## agent\n",
        "        num_states = self.env.observation_space.shape[0]    # position, velocity, angle, angular velocity\n",
        "        list_state_range = []\n",
        "        for i in range(num_states):\n",
        "            list_state_range.append([env.observation_space.low[i], env.observation_space.high[i]])\n",
        "        list_state_range[1] = [-3.0, 3.0]\n",
        "        list_state_range[3] = [-0.5, 0.5]\n",
        "        print(\"list_state_range = \", list_state_range)\n",
        "        list_state_reso = [4, 4, 6, 6]\n",
        "        num_actions = self.env.action_space.n\n",
        "\n",
        "        self.agent = Agent(num_states, list_state_range, list_state_reso, num_actions, gamma, r, lr)\n",
        " \n",
        "    def train(self):\n",
        "        num_completed_episodes = 0\n",
        "  \n",
        "        for episode in range(self.num_episodes):\n",
        "            obs = self.env.reset()\n",
        "            episode_reward = 0\n",
        " \n",
        "            for step in range(self.max_step):\n",
        "                ## get action\n",
        "                action = self.agent.getAction(obs, is_training=True)\n",
        "                ## observe next step\n",
        "                next_obs, _, is_done, _ = self.env.step(action)\n",
        "                ## get reward\n",
        "                if is_done:\n",
        "                    if step < max_step - 1:\n",
        "                        reward = -100\n",
        "                    else:\n",
        "                        reward = 1\n",
        "                        num_completed_episodes += 1\n",
        "                else:\n",
        "                    reward = 1\n",
        "                episode_reward += reward\n",
        "                ## update\n",
        "                self.agent.updateQtable(obs, action, reward, next_obs)\n",
        "                ## to next step\n",
        "                obs = next_obs\n",
        "\n",
        "                if is_done:\n",
        "                    print('{0} Episode: Finished after {1} time steps with reward {2}'.format(episode, step+1, episode_reward))\n",
        "                    break\n",
        "        print(\"num_completed_episodes = \", num_completed_episodes)\n",
        "\n",
        "    def evaluate(self):\n",
        "        obs = self.env.reset()\n",
        "        \n",
        "        for step in range(self.max_step):\n",
        "            ## get action\n",
        "            action = self.agent.getAction(obs, is_training=False)\n",
        "            ## observe next step\n",
        "            next_obs, _, is_done, _ = self.env.step(action)\n",
        "            ## to next step\n",
        "            obs = next_obs\n",
        "\n",
        "            if is_done:\n",
        "                print('Evaluation: Finished after {} time steps'.format(step+1))\n",
        "                break"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDKojCejb2-A"
      },
      "source": [
        "Prepare showing videos of the restults"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvmszieobbrX"
      },
      "source": [
        "def show_video(env):\n",
        "    for frame in env.videos:\n",
        "        print(\"frame = \", frame)\n",
        "        video = io.open(frame[0], 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "\n",
        "        display.display(display.HTML(data=\"\"\"\n",
        "            <video alt=\"test\" controls>\n",
        "            <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "            </video>\n",
        "            \"\"\".format(encoded.decode('ascii')))\n",
        "        )"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C7zmRMnW1i8"
      },
      "source": [
        "Run training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJqfOQ-tW0sA",
        "outputId": "8a908557-7c11-4cfc-8bda-e280ac3d813c"
      },
      "source": [
        "## parameters\n",
        "num_episodes = 500\n",
        "max_step = 200\n",
        "gamma = 0.9\n",
        "r = 0.99\n",
        "lr = 0.5\n",
        "\n",
        "## run\n",
        "cartpole_env = Environment(num_episodes, max_step, gamma, r, lr)\n",
        "cartpole_env.train()\n",
        "print(\"Evaluation\")\n",
        "cartpole_env.evaluate()\n",
        "show_video(cartpole_env.env)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "list_state_range =  [[-4.8, 4.8], [-3.0, 3.0], [-0.41887903, 0.41887903], [-0.5, 0.5]]\n",
            "0 Episode: Finished after 30 time steps with reward -71\n",
            "1 Episode: Finished after 15 time steps with reward -86\n",
            "2 Episode: Finished after 10 time steps with reward -91\n",
            "3 Episode: Finished after 12 time steps with reward -89\n",
            "4 Episode: Finished after 16 time steps with reward -85\n",
            "5 Episode: Finished after 14 time steps with reward -87\n",
            "6 Episode: Finished after 16 time steps with reward -85\n",
            "7 Episode: Finished after 16 time steps with reward -85\n",
            "8 Episode: Finished after 40 time steps with reward -61\n",
            "9 Episode: Finished after 9 time steps with reward -92\n",
            "10 Episode: Finished after 68 time steps with reward -33\n",
            "11 Episode: Finished after 39 time steps with reward -62\n",
            "12 Episode: Finished after 34 time steps with reward -67\n",
            "13 Episode: Finished after 10 time steps with reward -91\n",
            "14 Episode: Finished after 49 time steps with reward -52\n",
            "15 Episode: Finished after 31 time steps with reward -70\n",
            "16 Episode: Finished after 36 time steps with reward -65\n",
            "17 Episode: Finished after 39 time steps with reward -62\n",
            "18 Episode: Finished after 39 time steps with reward -62\n",
            "19 Episode: Finished after 67 time steps with reward -34\n",
            "20 Episode: Finished after 42 time steps with reward -59\n",
            "21 Episode: Finished after 62 time steps with reward -39\n",
            "22 Episode: Finished after 77 time steps with reward -24\n",
            "23 Episode: Finished after 87 time steps with reward -14\n",
            "24 Episode: Finished after 115 time steps with reward 14\n",
            "25 Episode: Finished after 71 time steps with reward -30\n",
            "26 Episode: Finished after 148 time steps with reward 47\n",
            "27 Episode: Finished after 42 time steps with reward -59\n",
            "28 Episode: Finished after 105 time steps with reward 4\n",
            "29 Episode: Finished after 92 time steps with reward -9\n",
            "30 Episode: Finished after 81 time steps with reward -20\n",
            "31 Episode: Finished after 76 time steps with reward -25\n",
            "32 Episode: Finished after 105 time steps with reward 4\n",
            "33 Episode: Finished after 92 time steps with reward -9\n",
            "34 Episode: Finished after 200 time steps with reward 200\n",
            "35 Episode: Finished after 125 time steps with reward 24\n",
            "36 Episode: Finished after 200 time steps with reward 200\n",
            "37 Episode: Finished after 200 time steps with reward 200\n",
            "38 Episode: Finished after 200 time steps with reward 200\n",
            "39 Episode: Finished after 147 time steps with reward 46\n",
            "40 Episode: Finished after 15 time steps with reward -86\n",
            "41 Episode: Finished after 200 time steps with reward 200\n",
            "42 Episode: Finished after 90 time steps with reward -11\n",
            "43 Episode: Finished after 133 time steps with reward 32\n",
            "44 Episode: Finished after 74 time steps with reward -27\n",
            "45 Episode: Finished after 190 time steps with reward 89\n",
            "46 Episode: Finished after 200 time steps with reward 200\n",
            "47 Episode: Finished after 185 time steps with reward 84\n",
            "48 Episode: Finished after 110 time steps with reward 9\n",
            "49 Episode: Finished after 128 time steps with reward 27\n",
            "50 Episode: Finished after 133 time steps with reward 32\n",
            "51 Episode: Finished after 13 time steps with reward -88\n",
            "52 Episode: Finished after 200 time steps with reward 200\n",
            "53 Episode: Finished after 130 time steps with reward 29\n",
            "54 Episode: Finished after 43 time steps with reward -58\n",
            "55 Episode: Finished after 122 time steps with reward 21\n",
            "56 Episode: Finished after 50 time steps with reward -51\n",
            "57 Episode: Finished after 63 time steps with reward -38\n",
            "58 Episode: Finished after 38 time steps with reward -63\n",
            "59 Episode: Finished after 141 time steps with reward 40\n",
            "60 Episode: Finished after 200 time steps with reward 200\n",
            "61 Episode: Finished after 200 time steps with reward 200\n",
            "62 Episode: Finished after 161 time steps with reward 60\n",
            "63 Episode: Finished after 68 time steps with reward -33\n",
            "64 Episode: Finished after 31 time steps with reward -70\n",
            "65 Episode: Finished after 200 time steps with reward 200\n",
            "66 Episode: Finished after 98 time steps with reward -3\n",
            "67 Episode: Finished after 200 time steps with reward 200\n",
            "68 Episode: Finished after 146 time steps with reward 45\n",
            "69 Episode: Finished after 200 time steps with reward 200\n",
            "70 Episode: Finished after 200 time steps with reward 200\n",
            "71 Episode: Finished after 200 time steps with reward 200\n",
            "72 Episode: Finished after 104 time steps with reward 3\n",
            "73 Episode: Finished after 179 time steps with reward 78\n",
            "74 Episode: Finished after 200 time steps with reward 200\n",
            "75 Episode: Finished after 15 time steps with reward -86\n",
            "76 Episode: Finished after 10 time steps with reward -91\n",
            "77 Episode: Finished after 15 time steps with reward -86\n",
            "78 Episode: Finished after 32 time steps with reward -69\n",
            "79 Episode: Finished after 200 time steps with reward 200\n",
            "80 Episode: Finished after 12 time steps with reward -89\n",
            "81 Episode: Finished after 133 time steps with reward 32\n",
            "82 Episode: Finished after 200 time steps with reward 200\n",
            "83 Episode: Finished after 200 time steps with reward 200\n",
            "84 Episode: Finished after 200 time steps with reward 200\n",
            "85 Episode: Finished after 75 time steps with reward -26\n",
            "86 Episode: Finished after 106 time steps with reward 5\n",
            "87 Episode: Finished after 200 time steps with reward 200\n",
            "88 Episode: Finished after 188 time steps with reward 87\n",
            "89 Episode: Finished after 200 time steps with reward 200\n",
            "90 Episode: Finished after 42 time steps with reward -59\n",
            "91 Episode: Finished after 90 time steps with reward -11\n",
            "92 Episode: Finished after 68 time steps with reward -33\n",
            "93 Episode: Finished after 121 time steps with reward 20\n",
            "94 Episode: Finished after 200 time steps with reward 200\n",
            "95 Episode: Finished after 182 time steps with reward 81\n",
            "96 Episode: Finished after 200 time steps with reward 200\n",
            "97 Episode: Finished after 200 time steps with reward 200\n",
            "98 Episode: Finished after 200 time steps with reward 200\n",
            "99 Episode: Finished after 200 time steps with reward 200\n",
            "100 Episode: Finished after 200 time steps with reward 200\n",
            "101 Episode: Finished after 200 time steps with reward 200\n",
            "102 Episode: Finished after 200 time steps with reward 200\n",
            "103 Episode: Finished after 200 time steps with reward 200\n",
            "104 Episode: Finished after 152 time steps with reward 51\n",
            "105 Episode: Finished after 9 time steps with reward -92\n",
            "106 Episode: Finished after 200 time steps with reward 200\n",
            "107 Episode: Finished after 32 time steps with reward -69\n",
            "108 Episode: Finished after 13 time steps with reward -88\n",
            "109 Episode: Finished after 56 time steps with reward -45\n",
            "110 Episode: Finished after 114 time steps with reward 13\n",
            "111 Episode: Finished after 9 time steps with reward -92\n",
            "112 Episode: Finished after 28 time steps with reward -73\n",
            "113 Episode: Finished after 114 time steps with reward 13\n",
            "114 Episode: Finished after 44 time steps with reward -57\n",
            "115 Episode: Finished after 16 time steps with reward -85\n",
            "116 Episode: Finished after 102 time steps with reward 1\n",
            "117 Episode: Finished after 200 time steps with reward 200\n",
            "118 Episode: Finished after 200 time steps with reward 200\n",
            "119 Episode: Finished after 200 time steps with reward 200\n",
            "120 Episode: Finished after 200 time steps with reward 200\n",
            "121 Episode: Finished after 200 time steps with reward 200\n",
            "122 Episode: Finished after 200 time steps with reward 200\n",
            "123 Episode: Finished after 118 time steps with reward 17\n",
            "124 Episode: Finished after 200 time steps with reward 200\n",
            "125 Episode: Finished after 200 time steps with reward 200\n",
            "126 Episode: Finished after 75 time steps with reward -26\n",
            "127 Episode: Finished after 195 time steps with reward 94\n",
            "128 Episode: Finished after 200 time steps with reward 200\n",
            "129 Episode: Finished after 200 time steps with reward 200\n",
            "130 Episode: Finished after 200 time steps with reward 200\n",
            "131 Episode: Finished after 130 time steps with reward 29\n",
            "132 Episode: Finished after 200 time steps with reward 200\n",
            "133 Episode: Finished after 200 time steps with reward 200\n",
            "134 Episode: Finished after 200 time steps with reward 200\n",
            "135 Episode: Finished after 200 time steps with reward 200\n",
            "136 Episode: Finished after 200 time steps with reward 200\n",
            "137 Episode: Finished after 197 time steps with reward 96\n",
            "138 Episode: Finished after 65 time steps with reward -36\n",
            "139 Episode: Finished after 68 time steps with reward -33\n",
            "140 Episode: Finished after 159 time steps with reward 58\n",
            "141 Episode: Finished after 60 time steps with reward -41\n",
            "142 Episode: Finished after 161 time steps with reward 60\n",
            "143 Episode: Finished after 85 time steps with reward -16\n",
            "144 Episode: Finished after 187 time steps with reward 86\n",
            "145 Episode: Finished after 104 time steps with reward 3\n",
            "146 Episode: Finished after 124 time steps with reward 23\n",
            "147 Episode: Finished after 59 time steps with reward -42\n",
            "148 Episode: Finished after 25 time steps with reward -76\n",
            "149 Episode: Finished after 59 time steps with reward -42\n",
            "150 Episode: Finished after 155 time steps with reward 54\n",
            "151 Episode: Finished after 112 time steps with reward 11\n",
            "152 Episode: Finished after 182 time steps with reward 81\n",
            "153 Episode: Finished after 60 time steps with reward -41\n",
            "154 Episode: Finished after 151 time steps with reward 50\n",
            "155 Episode: Finished after 200 time steps with reward 200\n",
            "156 Episode: Finished after 200 time steps with reward 200\n",
            "157 Episode: Finished after 200 time steps with reward 200\n",
            "158 Episode: Finished after 200 time steps with reward 200\n",
            "159 Episode: Finished after 200 time steps with reward 200\n",
            "160 Episode: Finished after 200 time steps with reward 200\n",
            "161 Episode: Finished after 193 time steps with reward 92\n",
            "162 Episode: Finished after 200 time steps with reward 200\n",
            "163 Episode: Finished after 124 time steps with reward 23\n",
            "164 Episode: Finished after 160 time steps with reward 59\n",
            "165 Episode: Finished after 200 time steps with reward 200\n",
            "166 Episode: Finished after 200 time steps with reward 200\n",
            "167 Episode: Finished after 200 time steps with reward 200\n",
            "168 Episode: Finished after 200 time steps with reward 200\n",
            "169 Episode: Finished after 200 time steps with reward 200\n",
            "170 Episode: Finished after 180 time steps with reward 79\n",
            "171 Episode: Finished after 100 time steps with reward -1\n",
            "172 Episode: Finished after 126 time steps with reward 25\n",
            "173 Episode: Finished after 29 time steps with reward -72\n",
            "174 Episode: Finished after 200 time steps with reward 200\n",
            "175 Episode: Finished after 123 time steps with reward 22\n",
            "176 Episode: Finished after 200 time steps with reward 200\n",
            "177 Episode: Finished after 200 time steps with reward 200\n",
            "178 Episode: Finished after 63 time steps with reward -38\n",
            "179 Episode: Finished after 96 time steps with reward -5\n",
            "180 Episode: Finished after 31 time steps with reward -70\n",
            "181 Episode: Finished after 200 time steps with reward 200\n",
            "182 Episode: Finished after 125 time steps with reward 24\n",
            "183 Episode: Finished after 145 time steps with reward 44\n",
            "184 Episode: Finished after 89 time steps with reward -12\n",
            "185 Episode: Finished after 119 time steps with reward 18\n",
            "186 Episode: Finished after 116 time steps with reward 15\n",
            "187 Episode: Finished after 89 time steps with reward -12\n",
            "188 Episode: Finished after 86 time steps with reward -15\n",
            "189 Episode: Finished after 85 time steps with reward -16\n",
            "190 Episode: Finished after 8 time steps with reward -93\n",
            "191 Episode: Finished after 30 time steps with reward -71\n",
            "192 Episode: Finished after 73 time steps with reward -28\n",
            "193 Episode: Finished after 148 time steps with reward 47\n",
            "194 Episode: Finished after 61 time steps with reward -40\n",
            "195 Episode: Finished after 130 time steps with reward 29\n",
            "196 Episode: Finished after 117 time steps with reward 16\n",
            "197 Episode: Finished after 193 time steps with reward 92\n",
            "198 Episode: Finished after 118 time steps with reward 17\n",
            "199 Episode: Finished after 200 time steps with reward 200\n",
            "200 Episode: Finished after 200 time steps with reward 200\n",
            "201 Episode: Finished after 200 time steps with reward 200\n",
            "202 Episode: Finished after 153 time steps with reward 52\n",
            "203 Episode: Finished after 42 time steps with reward -59\n",
            "204 Episode: Finished after 146 time steps with reward 45\n",
            "205 Episode: Finished after 200 time steps with reward 200\n",
            "206 Episode: Finished after 200 time steps with reward 200\n",
            "207 Episode: Finished after 183 time steps with reward 82\n",
            "208 Episode: Finished after 78 time steps with reward -23\n",
            "209 Episode: Finished after 181 time steps with reward 80\n",
            "210 Episode: Finished after 189 time steps with reward 88\n",
            "211 Episode: Finished after 53 time steps with reward -48\n",
            "212 Episode: Finished after 119 time steps with reward 18\n",
            "213 Episode: Finished after 133 time steps with reward 32\n",
            "214 Episode: Finished after 196 time steps with reward 95\n",
            "215 Episode: Finished after 155 time steps with reward 54\n",
            "216 Episode: Finished after 29 time steps with reward -72\n",
            "217 Episode: Finished after 43 time steps with reward -58\n",
            "218 Episode: Finished after 147 time steps with reward 46\n",
            "219 Episode: Finished after 146 time steps with reward 45\n",
            "220 Episode: Finished after 183 time steps with reward 82\n",
            "221 Episode: Finished after 200 time steps with reward 200\n",
            "222 Episode: Finished after 200 time steps with reward 200\n",
            "223 Episode: Finished after 200 time steps with reward 200\n",
            "224 Episode: Finished after 182 time steps with reward 81\n",
            "225 Episode: Finished after 200 time steps with reward 200\n",
            "226 Episode: Finished after 125 time steps with reward 24\n",
            "227 Episode: Finished after 141 time steps with reward 40\n",
            "228 Episode: Finished after 200 time steps with reward 200\n",
            "229 Episode: Finished after 143 time steps with reward 42\n",
            "230 Episode: Finished after 140 time steps with reward 39\n",
            "231 Episode: Finished after 107 time steps with reward 6\n",
            "232 Episode: Finished after 125 time steps with reward 24\n",
            "233 Episode: Finished after 140 time steps with reward 39\n",
            "234 Episode: Finished after 177 time steps with reward 76\n",
            "235 Episode: Finished after 152 time steps with reward 51\n",
            "236 Episode: Finished after 108 time steps with reward 7\n",
            "237 Episode: Finished after 25 time steps with reward -76\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mESpPTF72xIr"
      },
      "source": [
        "## References\n",
        "- [Getting Started with Gym](https://gym.openai.com/docs/)\n",
        "- [OpenAI Gym  Google Colab (2020.6)](https://qiita.com/ymd_h/items/c393797deb72e1779269)\n",
        "- [minnano_rl/section_2/01_simple_reinforcement_learning.ipynb](https://github.com/yukinaga/minnano_rl/blob/main/section_2/01_simple_reinforcement_learning.ipynb)\n",
        "- [10CartPoleQ](https://book.mynavi.jp/manatee/detail/id=88997)"
      ]
    }
  ]
}